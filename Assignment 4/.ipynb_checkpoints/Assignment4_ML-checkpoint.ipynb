{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb5ec893",
   "metadata": {
    "id": "cb5ec893"
   },
   "source": [
    "## 1. Anomaly Detection (30 points)\n",
    "\n",
    "**Part A (5 Points):**\n",
    "\n",
    "By dividing a data set into quartiles, IQR is used to measure variability. The data is sorted ascending and divided into four equal parts. Q1, Q2, Q3, also known as the first, second, and third quartiles, are the values that separate the four equal parts.\n",
    "\n",
    "Use the following data points to calculate outliers in the data\n",
    "data = [11, 3, 8, 10, 12, 5, 1, 50]\n",
    "\n",
    "Using a box plot, show the outliers in the box plot.\n",
    "\n",
    "**Part B (5 points):**\n",
    "\n",
    "Using the formula to calculate the Z-score detect outliers in the following data points.\n",
    "data = [6, 3, 9, 6, 9, 20, 3, 10, 3, 50, 6, 5, 9, 9, 3, 6, 3]\n",
    "Using a box plot, show the outliers in the box plot.\n",
    "\n",
    "**Part C (20 points):**\n",
    "\n",
    "Use the dataset attached for identifying the outliers using Z-score.\n",
    "\n",
    "Steps to follow in this question\n",
    "\n",
    "- Step1(5 points): Show outliers using histograms and scatterplots. Then\n",
    "\n",
    "- Step2(7 points): Identify the outliers using Z-score for SalePrice column by using atleast 4 different thresholds.\n",
    "\n",
    "- Step3(4 points): Print the number of outliers removed.\n",
    "\n",
    "- Step4(4 points): Use LocalOutlierFactor as discussed in the class to plot the outliers from SalePrice and LotArea columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fef8c2",
   "metadata": {
    "id": "49fef8c2"
   },
   "source": [
    "## 2. PCA (35 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c6f403",
   "metadata": {
    "id": "d4c6f403"
   },
   "source": [
    "Accuracy Comparison for Logistic Regression model: before and after PCA.\n",
    "Please follow the following steps:\n",
    "\n",
    "  1. Seperate and standardize the disease classification dataset. (5 points)\n",
    "  2. Do Eigen decomposition using any LA library of your choice. Display scree plot. (10 points)\n",
    "  3. Primary Component Selection. (Select the first 6 components) (5 points)\n",
    "  4. Projection in a New Feature Space. (5 points)\n",
    "  5. Principal Component Analysis. (5 points)\n",
    "  6. Compare the presision and recall for the data using logistic regression before and after PCA. (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e057e3ee",
   "metadata": {
    "id": "e057e3ee"
   },
   "source": [
    "## 3. EM Algorithm (35 points)\n",
    "\n",
    "Etimate the probability distribution in a 1-dimensional dataset\n",
    "There are two Normal distributions  𝑁(𝜇1,𝜎1^2) and 𝑁(𝜇2,𝜎2^2).\n",
    "There are 5 paramaters to estimate: 𝜃=(𝑤,𝜇1,𝜎1^2,𝜇2,𝜎2^2) where 𝑤 is the probability that the data comes from the first normal probability distribution and (1-𝑤) comes from the second normal probability distribution.\n",
    "The probability density function (PDF) of the mixture model is: 𝑓(𝑥|𝜃)=𝑤 𝑓1(𝑥 | 𝜇1,𝜎1^2)+(1−𝑤) 𝑓2(𝑥 | 𝜇2,𝜎2^2)\n",
    "Your goal is to best fit a given probability density by finding 𝜃=(𝑤,𝜇1,𝜎1^2,𝜇2,𝜎2^2) through EM iterations.\n",
    "\n",
    "Using the following way to produce data:\n",
    "```python\n",
    "import numpy as np\n",
    "random_seed=36784765\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "Mean1 = 9.0  # Input parameter, mean of first normal probability distribution\n",
    "Standard_dev1 = 5.0 #@param {type:\"number\"}\n",
    "Mean2 = 2.0 # Input parameter, mean of second normal  probability distribution\n",
    "Standard_dev2 = 2.0 #@param {type:\"number\"}\n",
    "\n",
    "# generate data\n",
    "y1 = np.random.normal(Mean1, Standard_dev1, 500)\n",
    "y2 = np.random.normal(Mean2, Standard_dev2, 2000)\n",
    "data=np.append(y1,y2)\n",
    "\n",
    "```\n",
    "\n",
    "(1) Using a single Gaussion to estimate and draw a picure to see the result: (5 points)\n",
    "```python\n",
    "class Gaussian:\n",
    "\"Model univariate Gaussian\"\n",
    "def __init__(self, mu, sigma):\n",
    "    #mean and standard deviation\n",
    "\n",
    "\n",
    "#probability density function\n",
    "def pdf(self, datum):\n",
    "    \"Probability of a data point given the current parameters\"\n",
    "```\n",
    "\n",
    "(2) Using a 2 Gaussian mixture model to estimate and draw a picture to see the result(Do not use sklearn GaussianMixture): (30 points)\n",
    "```python\n",
    "class GaussianMixture_self:\n",
    "\"Model mixture of two univariate Gaussians and their EM estimation\"\n",
    "\n",
    "def __init__(self, data, mu_min=min(data), mu_max=max(data), sigma_min=1, sigma_max=1, mix=.5):\n",
    "\n",
    "\n",
    "def Estep(self):\n",
    "    \"Perform an E(stimation)-step, assign each point to gaussian 1 or 2 with a percentage\"\n",
    "\n",
    "def Mstep(self, weights):\n",
    "    \"Perform an M(aximization)-step\"\n",
    "\n",
    "def iterate(self, N=1, verbose=False):\n",
    "    \"Perform N iterations, then compute log-likelihood\"\n",
    "\n",
    "def pdf(self, x):\n",
    "        \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a25b11",
   "metadata": {
    "id": "30a25b11"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
